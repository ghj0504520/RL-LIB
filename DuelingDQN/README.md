# Dueling DQN Algorithm
## Paper
* https://arxiv.org/abs/1511.06581
## Main Algorithm
* Same as DQN, the only difference is NN architecture
  * ![DuelingDQN-archi](DuelingDQN-archi.png)
  * $\Large Q(s,a,w)=V(s,w^V)+A(s,a,w^A)-mean_aA(s,a,w^A)$
  * å‡è¨­stateæ˜¯discreteï¼Œåªæœ‰å››å€‹ï¼Œè€Œactionåªæœ‰ä¸‰å€‹ï¼Œå…¶ğ‘„(s,a)ç”±ä¸€å€‹tableä¾†è¡¨ç¤ºã€‚ ğ‘‰(s)æ˜¯å°ä¸åŒstateéƒ½æœ‰ä¸€å€‹å€¼ï¼Œğ´(s,a)æ˜¯ä¸åŒstateå°ä¸åŒactionéƒ½æœ‰ä¸€å€‹å€¼ï¼Œå°‡ğ‘‰çš„å€¼åŠ åˆ°ğ´çš„æ¯ä¸€å€‹columnå°±å¯ä»¥å¾—åˆ°ğ‘„(s,a)å°±å¯ä»¥ã€‚ é€™éº¼åšçš„å¥½è™•åœ¨æ–¼ï¼Œå¦‚æœåªæœ‰æ›´å‹•æŸä¸€å€‹stateçš„å…©å€‹actionçš„å€¼ï¼Œè€Œæ¨¡å‹æœ€çµ‚æ±ºå®šæ›´å‹•çš„æ˜¯ğ‘‰(s)ï¼Œé‚£æœ€çµ‚å—å½±éŸ¿çš„ä¸åªæ˜¯å…©å€‹actionï¼Œæ‰€æ˜¯ä¸‰å€‹actionéƒ½æœƒå—åˆ°å½±éŸ¿ã€‚é€™æ„å‘³è‘—å³ä½¿ä½ æ²’æœ‰sampleéçš„actionä¹Ÿæœƒæœ‰ç›¸å°æ‡‰çš„å½±éŸ¿ï¼Œä¹Ÿæ²’æœ‰å¿…è¦å…¨éƒ¨çš„state-action pairéƒ½ä¸€å®šè¦sampleéï¼Œåªè¦ğ‘‰(s)æœ‰ç•°å‹•å°±å…¨éƒ¨é€šé€šç•°å‹•ã€‚
  * ç‚ºäº†é¿å…æ©Ÿå™¨æœ€å¾Œè®“æ‰€æœ‰ğ‘‰(s)éƒ½æ˜¯0è€Œé€ æˆğ‘„(s,a) = ğ´(s,a)ï¼Œå¯¦ä½œä¸Šæœƒå°ğ´(s,a)åšä¸€äº›ç´„æŸï¼Œè¿‘è€Œè®“æ©Ÿå™¨æ›´æ–°ğ‘‰(s)ï¼Œå¯¦ä½œä¸Šç´„æŸé …å¯ä»¥åšnormalization (mean)ã€‚
* ![DQN-Algorithm](DQN-algorithm.png)
## Figure Out
* Value-Based
* Model-Free
* OFF-Policy
* Per-episode training instead of per-step
* Dueling Q network architecture
* Hard copy every 100 step
* Epsilon greedy decay as episodes increase
* CUDA device usage
* Target evaluation without gradient back propagation (add model.eval)
* total_episodes = 20000
* batch_size = 256
* gamma      = 0.99
* replay_buffer capacity 10000
* ewma_reward usage
## Environment and Target Game
* gym: 0.26.2
* numpy: 1.26.4 
* pytorch: 2.0.1 
* environment: "CartPole-v1"
## Result
* ![DuelingDQN](Dueling_DQN_plot-whole.png)
